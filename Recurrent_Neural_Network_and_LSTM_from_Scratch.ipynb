{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Recurrent Neural Network and LSTM from Scratch**"
      ],
      "metadata": {
        "id": "Y9J2G0Nq76Yk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVk25bHrWJeV",
        "outputId": "964b3a46-7383-467d-b6c5-ae166bda4c47",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'deep-learning-specialization-coursera'...\n",
            "remote: Enumerating objects: 766, done.\u001b[K\n",
            "remote: Counting objects: 100% (181/181), done.\u001b[K\n",
            "remote: Compressing objects: 100% (158/158), done.\u001b[K\n",
            "remote: Total 766 (delta 32), reused 121 (delta 19), pack-reused 585 (from 1)\u001b[K\n",
            "Receiving objects: 100% (766/766), 172.44 MiB | 18.72 MiB/s, done.\n",
            "Resolving deltas: 100% (113/113), done.\n",
            "Updating files: 100% (558/558), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/arindam96/deep-learning-specialization-coursera.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/deep-learning-specialization-coursera/Course 5 - Sequence Models/Week 1 - Recurrent Neural Networks/2. Programming Assignment Building your Recurrent Neural Network - Step by Step"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nf54u3sMXIB9",
        "outputId": "8c5a9672-cec8-4ff1-b0db-22a27e057173",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deep-learning-specialization-coursera/Course 5 - Sequence Models/Week 1 - Recurrent Neural Networks/2. Programming Assignment Building your Recurrent Neural Network - Step by Step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIZf7H6gXjd_",
        "outputId": "99a0eb74-5bcf-4311-a1a2-3552cae0423a",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building_a_Recurrent_Neural_Network_Step_by_Step.ipynb\t__pycache__\n",
            "generateTestCases.py\t\t\t\t\trnn_utils.py\n",
            "images\t\t\t\t\t\t\ttest_utils.py\n",
            "public_tests.py\t\t\t\t\t\tutils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Neural Network\n",
        "\n",
        "We'll implement Recurrent Neural Network, or RNN and LSTM in Numpy!\n",
        "\n"
      ],
      "metadata": {
        "id": "bnlLtePlX1-6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table of Content\n",
        "\n",
        "- [Packages](#0)\n",
        "- [1 - Forward Propagation for Basic Recurrent Neural Network](#1)\n",
        "    - [1.1 - RNN cells](#1-1)\n",
        "    - [2.2 - RNN Forward Pass](#1-2)\n",
        "- [2 - Long Sort-Term Memory (LSTM) Network](#2)\n",
        "    - [2.1 - LSTM Cell](#2-1)\n",
        "    - [2.2 - Forward Pass for LSTM](#2-2)\n",
        "    "
      ],
      "metadata": {
        "id": "ibudEWZaYxGN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='0'></a>\n",
        "## Packages"
      ],
      "metadata": {
        "id": "bbhB8ff0cPT7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from rnn_utils import *"
      ],
      "metadata": {
        "id": "-u-kpeS6Xk0C"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='1'></a>\n",
        "## 1 - Forward Propagation for the Basic Recurrent Neural Network\n"
      ],
      "metadata": {
        "id": "nFbtbXr4csiY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='1-1'></a>\n",
        "### 1.1 - RNN Cell\n"
      ],
      "metadata": {
        "id": "E3xxlG-Nqn3b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### rnn_cell_forward"
      ],
      "metadata": {
        "id": "rY1KCNEpq31u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rnn_cell_forward(xt, a_prev, parameters):\n",
        "  Wax=parameters[\"Wax\"]\n",
        "  Waa=parameters[\"Waa\"]\n",
        "  Wya=parameters[\"Wya\"]\n",
        "  ba=parameters[\"ba\"]\n",
        "  by=parameters[\"by\"]\n",
        "\n",
        "  # Compute next activation state\n",
        "  a_next=np.tanh(np.dot(Waa, a_prev)+np.dot(Wax, xt)+ba)\n",
        "\n",
        "  # Compute output of the current cell\n",
        "  yt_pred=softmax(np.dot(Wya, xt)+by)\n",
        "\n",
        "  # Store values that we need to store in cache for backpropagation\n",
        "  cache=(a_next, a_prev, xt, parameters)\n",
        "\n",
        "  return a_next, yt_pred, cache"
      ],
      "metadata": {
        "id": "TMxHW_-Icg5D"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='1-2'></a>\n",
        "### 1.2 - RNN Forward Pass\n",
        "\n"
      ],
      "metadata": {
        "id": "7kjxC87hs7ZM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### rnn_forward"
      ],
      "metadata": {
        "id": "XZoByy9Ftm_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rnn_forward(x, a0, parameters):\n",
        "\n",
        "  # Initialize \"caches\" which will contain all the list of caches\n",
        "  cache=[]\n",
        "\n",
        "  # Retreive dimensions from shapes of x and paramerters[\"Wya\"]\n",
        "  n_x, m, T_x=x.shape\n",
        "  n_y, n_a=parameters[\"Wya\"].shape\n",
        "\n",
        "  # Initialize \"a\" and \"y_pred\" with zeros\n",
        "  a=np.zeros((n_a, m, T_x))\n",
        "  y_pred=np.zeros((n_y, m, T_x))\n",
        "\n",
        "  # Initialize a_next\n",
        "  a_next=a0\n",
        "\n",
        "  for t in range(T_x):\n",
        "    # Update net hidden state, compute prediction, get the cache\n",
        "    a_next, yt_pred, cache=rnn_cell_forward(x[:, :, t], a_next, parameters)\n",
        "    # Save the value of the new \"nexr\" hidden state\n",
        "    a[:, :, t]=a_next\n",
        "    # Save the value of prediction in y\n",
        "    y_pred[:, :, t]=yt_pred\n",
        "    # Append \"cache\" to \"caches\"\n",
        "    caches.append(cache)\n",
        "\n",
        "  caches=(caches, x)\n",
        "\n",
        "  return a, y_pred, caches\n"
      ],
      "metadata": {
        "id": "vE1APz3rswuC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***We've successfully built the forward propagation of a recurrent neural network from scratch!***"
      ],
      "metadata": {
        "id": "TvP-G9BxyzMW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='2'></a>\n",
        "## 2 - Long Short-Term Memory(LSTM) Network\n",
        "\n"
      ],
      "metadata": {
        "id": "mKJmpX56zmap"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='2.1'></a>\n",
        "### LSTM Cell\n",
        "\n",
        "### lstm_cell_forward"
      ],
      "metadata": {
        "id": "E9l_zzEN1yWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lstm_cell_forward(xt, a_prev, c_prev, parameters):\n",
        "\n",
        "  # Retrieve parameters from \"parameters\"\n",
        "  Wf=parameters[\"Wf\"]\n",
        "  bf=parameters[\"bf\"]\n",
        "  Wi=parameters[\"Wi\"]\n",
        "  bi=parameters[\"bi\"]\n",
        "  Wc=parameters[\"Wc\"]\n",
        "  bc=parameters[\"bc\"]\n",
        "  Wo=parameters[\"Wo\"]\n",
        "  bo=parameters[\"bo\"]\n",
        "  Wy=parameters[\"Wy\"]\n",
        "  by=parameters[\"by\"]\n",
        "\n",
        "  # Retrieve dimensions from shapes of xt and Wy\n",
        "  n_x, m=xt.shape\n",
        "  n_y, n_a=Wy.shape\n",
        "\n",
        "  # Concatenate a_prev and x_t\n",
        "  concat=np.concatenate((a_prev, xt), axis=0)\n",
        "\n",
        "  # Compute values of ft, it, ot, cct, c_next, a_next\n",
        "  ft=sigmoid(np.dot(Wf, concat)+bf)\n",
        "  it=sigmoid(np.dot(Wi, concat)+bi)\n",
        "  cct=np.tanh(np.dot(Wc, concat)+bc)\n",
        "  c_next=ft*c_prev+it*cct\n",
        "  ot=sigmoid(np.dot(Wo, concat)+bo)\n",
        "  a_next=ot*np.tanh(c_next)\n",
        "\n",
        "  # Compute prediction of the LSTM Cell\n",
        "  yt_pred=softmax(np.dot(Wy, a_next)+by)\n",
        "\n",
        "  # Store values for backward propagation in cache\n",
        "  cache=(a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters)\n",
        "\n",
        "  return a_next, c_next, yt_pred, cache"
      ],
      "metadata": {
        "id": "vHgU5Ie7yun2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='2-2'></a>\n",
        "### 2.2 - Forward Pass for LSTM\n",
        "\n",
        "### lstm_forward"
      ],
      "metadata": {
        "id": "wj1MkMKZ5LWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lstm_forward(x, a0, parameters):\n",
        "\n",
        "  # Initialize \"caches\", which will track list of all caches\n",
        "  caches=[]\n",
        "\n",
        "  Wy=parameters[\"Wy\"]\n",
        "\n",
        "  # Retrieve dimensions from shapes of x and parameters[\"Wy\"]\n",
        "  n_x, m, T_x=x.shape\n",
        "  n_y, n_a=Wy.shape\n",
        "\n",
        "  # Initialize \"a\", \"c\" and \"y\" with zeros\n",
        "  a=np.zeros((n_a, m, T_x))\n",
        "  c=np.zeros((n_a, m, T_x))\n",
        "  y=np.zeros((n_y, m, T_x))\n",
        "\n",
        "  # Loop over all time steps\n",
        "  for t in range(T_x):\n",
        "    # Get the 2D slice'xt' from 3D input 'x' at time step 't'\n",
        "    xt=x[:, :, t]\n",
        "\n",
        "    # Update next hidden state, next memory state, compute the prediction, get the cache\n",
        "    a_next, c_next, yt, cache=lstm_cell_forward(xt, a_next, c_next, parameters)\n",
        "    # Save the value of the new \"next\" hidden state in a (≈1 line)\n",
        "    a[:,:,t] = a_next\n",
        "    # Save the value of the next cell state (≈1 line)\n",
        "    c[:,:,t]  = c_next\n",
        "    # Save the value of the prediction in y (≈1 line)\n",
        "    y[:,:,t] = yt\n",
        "    # Append the cache into caches (≈1 line)\n",
        "    caches.append(cache)\n",
        "\n",
        "  # store values needed for backward propagation in cache\n",
        "  caches = (caches, x)\n",
        "\n",
        "  return a, y, c, caches"
      ],
      "metadata": {
        "id": "pG5bj-765IUH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***We'v Sucessfully implemented forward passes for both the Basic RNN and LSTM!***"
      ],
      "metadata": {
        "id": "Lv4QMKyk7gaq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AmOcUl3K6dfK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}